---
title: "05-MLR-Part2"
output: pdf_document
---

Objectives: 

1. Review MLR with continuous and categorical variable 

2. Fit a MLR with interaction term with continuous variables 

3. Fit a MLR with interaction terms with continuous and categorical variable 

4. Fit a MLR with 3-way interaction term and 2-way interaction terms 

# Load packages 

```{r}
library(tidyverse) # data manipulation 
library(ggplot2)   # plotting
```

# Load data 

```{r}
data <- read.csv("train.csv")
```

# 1. Review MLR with continuous and categorical variable

Refer to **04-MLR.Rmd** (**01-EDA-02.Rmd** for now) or proceed to 2. 


# 2. Fit a MLR with interaction term with continuous variables

Recall that in fit2, we fit `TotalBsmtSF` and `LotArea`. What if we have reason to suspect that the effect of `TotalBsmtSF` on `SalePrice`, and vice versa? 

fit2 is also referred to as an additive model since the effect is additive. It is also known as the parallel- or same-slopes model. 

```{r}
fit2 <- lm(SalePrice ~ TotalBsmtSF + LotArea, data = data)
summary(fit2)
```

fit8 extends fit2 to include an interaction term. fit8 is referred to as the non-additive model. It is also known as different-slopes model. 

```{r}
fit8_2 <- lm(SalePrice ~ TotalBsmtSF * LotArea, data = data)
summary(fit8_2)
```

```{r}
fit8 <- lm(SalePrice ~ TotalBsmtSF + LotArea + TotalBsmtSF:LotArea, data = data)
summary(fit8)
```

The p-val associated with `TotalBsmtSF:LotArea` is significant (`p-val = <2e-16`). Thus, we reject the null hypothesis Ho that $\beta_3 = 0$ and conclude that the effect of `TotalBsmtSF` on `SalePrice` depends on `LotArea`, and vice versa. 

Note again that the fit is poor (`R-squared: 0.06111`, `Adjusted R-squared: 0.05725`). 


# 3. Fit a MLR with interaction terms with continuous and categorical variable 

Recall that fit7 fits a continuous variable and a categorical variable. Can you guess how many parameters in total if we were to fit an interaction term? 

Notice that in **01-EDA-02.Rmd** we briefly cleaned the data and rename the data set as data_sub already. We need to do additional cleaning later but let's ignore it for now. 

```{r}
#Piping - manipulates and displays data but does not actually change and save data
data$MSZoning %>% str()
```

```{r}
data$MSZoning <- factor(data$MSZoning)
```

```{r}
data$MSZoning %>% str()
```

```{r}
levels(data$MSZoning)
```

fit7 has a total of parameters: 1 ($\beta_0$) for the intercept + 1 ($\beta_1$) for `TotalBsmtSF` + (5 - 1) ($\beta_2$, $\beta_3$, $\beta_4$, $\beta_5$) for each level/group of `MSZoning` excluding the reference group. 

```{r}
fit7 <- lm(SalePrice ~ TotalBsmtSF + MSZoning, data = data)
summary(fit7)
```

fit9 extends fit 7 to include an interaction term. 

```{r}
fit9 <- lm(SalePrice ~ TotalBsmtSF + MSZoning + TotalBsmtSF:MSZoning, data = data)
summary(fit9)
```

The model is given as 
$$
\mu(SalePrice | TotalBsmtF, MSZoning) = \beta_0 + \beta_1 TotalBasmtSF + \beta_2 MSZoningFV + ... \\ 
\beta_6 TotalBasmtSF \times MSZoningFV + \beta_7 TotalBasmtSF \times MSZoningRH + \beta_8 TotalBasmtSF \times MSZoningRL + \beta_9 TotalBasmetSF \times MSZoningRM 
$$

5 groups (5 regression lines if we were to plot it)

1st group (reference group)

$$
\mu(...) = \beta_0 + \beta_1 TotalBasmtSF + 0 
$$

2nd group (MSZoningFV = 1, the rest of the indicator variables = 0)

$$
\mu(...) = \beta_0 + \beta_1 TotalBasmtSF + \beta_2 * 1 + \beta_6 * TotalbasmSF \\
= (\beta_0 + \beta_2) + (\beta_1 + \beta_6) TotalbasmSF 
$$

3rd group (MSZoningRH = 1, the rest of the indicator variables = 0)

$$
... = (\beta_0 + \beta_3) + (\beta_1 + \beta7)TotalbasSF
$$

# 4. Fit a MLR with 3-way interaction term and 2-way interaction terms 

```{r}
data %>% glimpse
```

Instead of using `:`, we use `*` to shorten the line of code. Check how many parameters fit10 has? 

```{r}
fit10 <- lm(SalePrice ~ TotalBsmtSF * LotArea * GarageArea, data = data)
summary(fit10)
```

Here we only have 3 predictor variables. Now imagine if we fit a model with 81 predictor variables... 

0. Model diagnostics 
1. Model comparison (compare two models) 
2. Model selection (select from a lot of models) 
3. Feature selection (Continue to do EDA, apply PCA, random forest, etc) 
4. Prediction 
